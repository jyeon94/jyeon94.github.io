---
title: "R 머신러닝 데이터 분석4"
tags: [R, TIL]
style:
color:
description: 나성호 강사님 강의안 복습
---

### 데이터 표준화(Standardization)

#### 정의

- 머신러닝에서 알고리즘에서 거리 계산이 포함된 경우, 반드시 데이터 전처리 과정에서 데이터 표준화를 
  해줘야 한다
- 데이터 표준화의 이유는 입력변수마다 척도가 다른 경우, 척도가 상대적으로 큰 변수에 거리에 더 영향을 
  미치기 때문이다
- 데이터 표준화는 데이터 x에 대해 평균이 0이고, 표준편차가 1인 표준정규분포를 따르는 z-score로 변환하는 것을 의미한다
  - z-score는 평균값과 얼마나 거리가 먼지 계산해주는 통계적인 예측값이다
  - z = x-μ(평균)/σ(표준편차)

#### 데이터 정규화

- 데이터 표준화된 변수는 - 무한대 ~ 무한대 사이의 값을 가질 수 있지만, 0~1 사이의 값을 가지도록 데이터 정규화를 실행할 수 있다
- 데이터 정규화를 실행하는 경우, 신경망 알고리즘의 학습을 빠르게 수행할 수 있고, 지역 최적(Local optimum)에 빠지는 가능성을 줄일 수 있다는 장점이 있다
  - 지역 최적이란 이웃 후보 솔루션 세트 내에서 최적인 솔루션이다
- 데이터 정규화 공식은 표준화 공식에서 분자에는 μ 대신 '최솟값을' 지정하고, 분모에는 σ 대신 "최대값 - 최솟값을" 입력한다 
  - x-normalized = x - min / max-min

#### 표준화와 정규화 관련함수

- R 기본함수인 scale() 함수를 이용하면 표준화와 정규화를 실행할 수 있다.
- scale() 함수의 주요 인자는 다음과 같다
  - x: 변환하려는 숫자형 벡터를 할당한다
  - center: 표준화인 경우 x의 평균을, 정규화에는 x의 최솟값을 할당한다
  - scale: 표준화인 경우 x의 표준편차를, 정규화에는 x의 최대값과 최솟값의 차이를 할당한다

```R
set.seed(seed=1234)
heights <- rnorm(n=1000,mean=172.4,sd=5.7)
# 가상의 키 데이터를 표준화한다
scaled1 <- scale(x=heights) # center 인자에는 평균, scale 인자에는 표준편차가 기본값으로 할당된다
summary(object = scaled1) # scaled1의 평균이 0이다
sd(x=scaled1) # scaled1의 표준편차는 1이다
scaled2<- scale(x=heights,
               center = min(heights), #center 인자에 최솟값을 할당한다
               scale = max(heights) - min(heights)) # scale 인자에 최대값가 최솟값의 차이를
                                                    # 할당한다
summary(object=scaled2) #scale2의 평균은 0은 아니지만 최솟값이0, 최댓값이 1이다
sd(x= scaled2)
```



### 다양한 거리 계산법

#### 비유사성의 척도

- 머신러닝 알고리즘 중 일부는 데이터 row간 유사도 혹은 비유사도를 측정하는 경우가 있다. 유사도의 기준으로는 주로 거리(distance)를 이용한다
  - 군집분석의 경우, 레코드 간 거리가 가까울수록 유사도가 높다고 판단하여 같은 근접으로 묶는다
- 지도학습의 knn(k-최근접이웃) 알고리즘 또한 레코드 간 거리를 측정하고 가장 가까운 이웃들의 목표변수를 기준으로 분류모형은 다수결로 다수 범주의 라벨을, 회귀모형은 평균을 제시한다
- 비지도학습의 collaborative filtering(협업 필터링) 알고리즘도 레코드 간 유사도 측정 방법의 하나로 유클리드 거리를 사용한다

#### 거리의 개념

- 두 개의 점(관측값)간 거리를 계산하여, 그 거리가 짧을 수록 서로 유사한 특성을 갖는다고 판단
- 컬럼 간 척도가 다를 때 거리를 계산하면 척도가 큰 컬럼에 의해 크게 좌우되므로, 거리 계산을 하기에 앞서 데이터 표준화를 실행해야 한다
- 거리의 종류에는 맨하탄 거리, 유클리드 거리, 민코프스키 거리, 맥시멈 거리 등이 있다. 거리 계사는 dist() 함수를 사용하며, 'method' 인자에 거리의 종류를 할당하면 해당 거리를 반환한다

#### 거리의 특징

- 함수 d(x,y)를 두 점 x와 y의 거리를 반환하는 함수라고 가정했을 때, d(x,y)는 다음과 같은 특징을 갖는다
  - 모든 거리는 0보다 크거나 같다
    - d(x,y) ≥ 0
  - 교환법칙이 성립한다
    - d(x,y) = d(y,x)
  - 다른 한 점 z를 경유하는 거리의 합보다 작거나 같다. (삼각부등식)
    - d(x,y) ≤ d(x,z) +  d(y,z)

#### 거리의 종류

대표적인 거리 종류 몇 개만 알아보자

##### 거리의 종류1: 맨하탄 거리

- n차원의 공간에 두 점 A(x1,x2,---,xn)와 B(y1,y2,---,yn)가 존재한다고 가정할 때, 두 점 사이의 거리는 종류에 따라 다음과 같이 계산할 수 있다
- 맨하탄 거리는 두 점 A와 B의 차의 절대값을 모두 더한 값으로 "L1 norm"이라고 한다. 격자 모양의 거리를 자동차로 운행하는 모습을 연상하면 된다
  - 맨하탄 거리는 "택시캡 거리"라는 별칭을 갖고 있다.

##### 거리의 종류2: 유클리드 거리

- 유클리드 거리는 두 점 A와 B의 차를 제곱하여 모두 더한 값의 양의 제곱근이다 "L2 norm"이라고 한다
- 유클리드 거리는 가장 익수한 거리 계산 방법이며 두 점간 직선거리를 계산한 것이다

##### 거리의 종류3: 민코스프키 거리

- 민코프스키 거리는 n차원 민코프스키 공간에서의 거리를 의미한다
- 민코프스키 거리는 "Lp norm"이라고도 하는데, 앞서 언급했던 맨하탄 거리와 유클리드 거리를 일반화한 것이라고 생각하면 된다



